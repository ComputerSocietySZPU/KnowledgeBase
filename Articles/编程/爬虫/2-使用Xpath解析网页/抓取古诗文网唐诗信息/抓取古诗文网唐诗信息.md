源码：![6_抓取唐诗.py](./抓取古诗文网唐诗信息.assert/1746936889546-22c66408-1c17-402c-8436-889f0697937e.py)

## 一、教程目标
本教程将详细讲解如何使用Python编写一个爬虫程序，从古诗文网（https://so.gushiwen.cn/）抓取唐诗的相关信息，包括诗名、作者、朝代、诗歌原文、译文及注释、赏析、创作背景、简析等内容，并将这些数据保存到CSV文件中。通过本教程的学习，你将掌握以下技能：

- 使用```requests
```
库发送HTTP请求获取网页内容
- 使用```BeautifulSoup
```
库解析HTML文档并提取目标数据
- 使用```pandas
```
库处理和存储数据到CSV文件
- 处理URL路径拼接及相对路径转换为绝对路径
- 对网页数据进行清洗和整理
## 二、环境准备
在开始编写代码之前，请确保已经安装了以下Python库：

```bash
pip install requests chardet pandas beautifulsoup4
```
这些库的作用分别为：

- ```requests
```
：用于发送HTTP请求，获取网页数据。
- ```chardet
```
：自动检测网页的编码格式，避免中文乱码问题。
- ```pandas
```
：用于数据处理和存储，方便将数据保存为CSV文件。
- ```BeautifulSoup
```
：解析HTML文档，通过标签和CSS选择器提取所需数据。
- ```urllib.parse
```
中的```urljoin
```
：用于拼接URL，将相对路径转换为绝对路径。
## 三、核心代码解析
### 1. 获取网页内容

```python
def get_html(url, params = None, timeout = 7):
    print(f'正在抓取页面:{url}, params = {params}')
    headers = {
        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0'
    }
    try:
        response = requests.get(url,
                                params=params,
                                headers=headers,
                                timeout=timeout)
        response.raise_for_status()
        response.encoding = chardet.detect(response.content)['encoding']
        return response.text
    except Exception as e:
        print(e)
        return None
    finally:
        time.sleep(2)
```
**代码说明：**

- **请求头设置**：通过设置```User-Agent
```
伪装成浏览器，模拟正常用户访问，避免被网站识别为爬虫而拒绝访问。
- **异常处理**：使用```try-except
```
捕获请求过程中的异常，如网络错误、服务器返回错误状态码等，保证程序不会因请求失败而崩溃。
- **编码处理**：利用```chardet
```
自动检测网页编码，并设置```response.encoding
```
，确保获取的网页内容中文显示正常。
- **请求间隔**：在```finally
```
块中添加```time.sleep(2)
```
，每次请求后暂停2秒，降低请求频率，防止触发网站的反爬机制。### 2. 保存数据到CSV文件

```python
def save_data(data, file_name):
    pd_data = pd.DataFrame(data,
                           columns='title, author, dynasty, poem, yiwen, shangxi, beijing, jianxi'.split(','))
    if not os.path.exists(file_name): 
        pd_data.to_csv(file_name,
                       encoding='utf-8-sig',
                       index=False,
                       header=True)
    else:
        pd_data.to_csv(file_name,
                       encoding='utf-8-sig',
                       index=False,
                       header=False, 
                       mode = 'a+' 
                       )
```
**代码说明：**

- **数据结构化**：使用```pandas
```
的```DataFrame
```
将数据整理为表格形式，并指定列名。
- **文件存在判断**：通过```os.path.exists(file_name)
```
判断文件是否存在，如果不存在，则首次写入数据时添加表头；如果存在，则以追加模式写入数据，避免覆盖已有内容。
- **编码与存储**：采用```utf-8-sig
```
编码保存数据，防止CSV文件在Excel中打开时出现中文乱码问题，同时设置```index=False
```
不保存行索引。### 3. 提取单篇古诗详细信息

```python
def get_pome_data(url):
    html = get_html(url)
    if not html:
        return None
    bs = BeautifulSoup(html, 'lxml')
    id = url.split('_')[-1][:-5]
    # 诗名
    title_tag = bs.select_one('#zhengwen{} > h1'.format(id))
    title = title_tag.text if title_tag else None
    # 作者
    author_tag = bs.select_one('#zhengwen{} > p > a:nth-child(1)'.format(id))
    author = author_tag.text if author_tag else None
    # 朝代
    dynasty_tag = bs.select_one('#zhengwen{} > p > a:nth-child(2)'.format(id))
    dynasty = dynasty_tag.text if dynasty_tag else None
    # 诗歌
    poem_tag = bs.select_one('#contson{}'.format(id))
    poem = poem_tag.text.strip() if poem_tag else None

    divs = bs.select('body > div.main3 > div.left > div.sons')
    yiwen = None
    shangxi = None
    beijing = None
    jianxi = None
    for div in divs:
        type = div.select_one('div.contyishang > div > h2 > span')
        if not type:
            continue
        if "译文及注释" in type.text:
            p_tags = div.select('div.contyishang > p')
            yiwen = "\n".join([tag.text for tag in p_tags])
        elif "赏析" in type.text:
            p_tags = div.select('div > p')
            shangxi = "\n".join([tag.text for tag in p_tags])
        elif "创作背景" in type.text:
            p_tags = div.select('div.contyishang > p')
            beijing = "\n".join([tag.text for tag in p_tags])
        elif "简析" in type.text:
            p_tags = div.select('div.contyishang > p')
            jianxi = "\n".join([tag.text for tag in p_tags])
    return title, author, dynasty, poem, yiwen, shangxi, beijing, jianxi
```
**代码说明：**

- **页面获取与解析**：调用```get_html
```
函数获取网页内容，若获取失败则直接返回```None
```
；使用```BeautifulSoup
```
解析HTML文档。
- **数据定位与提取**：
- 通过CSS选择器（如```#zhengwen{} > h1
```
）定位诗名、作者、朝代、诗歌原文等标签，并提取对应文本内容。
- 对于译文及注释、赏析、创作背景、简析等内容，遍历包含这些信息的```<div>
```
标签，通过标签内的文本内容判断类型，再提取具体段落文本，使用```\n
```
连接成完整内容。
- **数据清洗**：使用```strip()
```
方法去除诗歌原文等文本中的多余空白字符。### 4. 主程序逻辑

```python
if __name__ == '__main__':
    base_url = r'https://so.gushiwen.cn/'
    url = urljoin(base_url, r'gushi/tangshi.aspx')
    # 爬取诗歌主页
    html = get_html(url)
    if html is None:
        sys.exit()
    bs = BeautifulSoup(html, 'lxml')
    link_tags = bs.select('body > div.main3 > div.left > div.sons > div:nth-child(1) > span > a')
    links = [tag.get('href') for tag in link_tags]
    data = []
    for link in links:
        url = urljoin(base_url, link)
        title, author, dynasty, poem, yiwen, shangxi, beijing, jianxi = get_pome_data(url)
        data.append((title, author, dynasty, poem, yiwen, shangxi, beijing, jianxi))
    # 保存数据
    save_data(data, 'pome.csv')
```
**代码说明：**

- **URL拼接**：定义```base_url
```
作为基础域名，使用```urljoin
```
将相对路径（如```gushi/tangshi.aspx
```
）拼接成完整的绝对URL。
- **主页数据提取**：获取唐诗主页的HTML内容，解析后通过CSS选择器提取每首诗详情页的链接。
- **详情页数据抓取**：遍历诗的链接列表，对每个链接调用```get_pome_data
```
函数获取详细信息，并将数据整理为元组形式添加到```data
```
列表中。
- **数据存储**：调用```save_data
```
函数将所有古诗数据保存到```pome.csv
```
文件中。
## 四、注意事项

1. **遵守法律法规**：在进行网络数据抓取时，需遵守《网络安全法》及网站的```robots.txt
```
协议，不得恶意抓取数据或侵犯他人权益。
2. **控制抓取频率**：避免过于频繁地发送请求，防止对目标网站服务器造成压力，引发反爬机制或封禁IP。
3. **网站结构变化**：目标网站的HTML结构可能会更新，若发现抓取数据异常，需及时检查和调整CSS选择器。
4. **数据存储限制**：CSV文件有一定的存储限制，若数据量过大，可考虑使用数据库（如SQLite、MySQL）进行存储。通过本教程，你已经掌握了从网页抓取古诗数据并保存的完整流程，可在此基础上根据实际需求进行扩展和优化。
