源码：![5. 爬取ChinaDaily新闻数据.py](./抓取中国日报时政新闻.assert/1746936644805-7fbd599a-43bd-4604-9d1d-7715b7fb582b.py)

## 一、教程目标
本教程将详细介绍如何使用Python编写一个爬虫程序，从中国日报网站抓取时政新闻，包括新闻标题、图片链接、摘要及正文内容，并将数据保存至CSV文件。通过本教程，你将掌握以下技能：

- 发送HTTP请求获取网页内容并处理编码问题
- 解析HTML页面提取目标数据
- 实现页面跳转抓取及自动翻页功能
- 清洗和处理新闻内容
- 数据持久化存储至CSV文件
## 二、环境准备
在开始编写代码前，确保已安装以下Python库：

```bash
pip install requests chardet pandas beautifulsoup4
```
各库功能说明：

- ```requests
```
：用于发送HTTP请求获取网页数据
- ```chardet
```
：自动检测网页编码，避免中文乱码问题
- ```pandas
```
：用于处理和保存数据到CSV文件
- ```BeautifulSoup
```
：解析HTML文档，方便提取标签内容
- ```urllib.parse
```
：处理URL拼接，确保相对路径转换为绝对路径
## 三、核心代码解析
### 1. 获取网页内容

```python
def get_html(url, params = None, timeout = 3):
    print(f'正在抓取页面:{url}, params = {params}')
    headers = {
        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0'
    }
    try:
        response = requests.get(url,
                                params=params,
                                headers=headers,
                                timeout=timeout)
        response.raise_for_status()
        response.encoding = chardet.detect(response.content)['encoding']
        return response.text
    except Exception as e:
        print(e)
        return None
    finally:
        time.sleep(3)
```
**关键逻辑：**

- 设置```User-Agent
```
模拟浏览器访问，降低被封禁风险
- 使用```chardet
```
自动检测网页编码（如GBK、UTF-8）
- 通过```raise_for_status()
```
检查HTTP响应状态码（如404、500）
- 添加```time.sleep(3)
```
防止高频请求触发反爬机制
### 2. 保存数据到CSV文件

```python
def save_data(data, file_name):
    pd_data = pd.DataFrame(data,
                           columns='标题 图片url 摘要 内容'.split())
    if not os.path.exists(file_name):
        pd_data.to_csv(file_name,
                       header=True,
                       index=False,
                       encoding='utf-8-sig')
    else:
        pd_data.to_csv(file_name,
                       header=False,
                       index=False,
                       encoding='utf-8-sig',
                       mode = 'a+') 
```
**关键逻辑：**

- 使用```pandas
```
的```DataFrame
```
结构化存储数据
- 通过```utf-8-sig
```
编码确保中文正确写入CSV
- ```mode='a+'
```
实现数据追加，避免覆盖已有内容
### 3. 提取新闻正文内容

```python
def get_news_content(content_html):
    bs = BeautifulSoup(content_html, 'lxml')
    content_tags = bs.select('#Content > p')
    content = [p_tag.text for p_tag in content_tags]
    return '\n'.join(content)
```
**关键逻辑：**

- 使用CSS选择器```#Content > p
```
定位新闻正文段落
- 提取每个```<p>
```
标签内的文本，用换行符拼接为完整内容
### 4. 提取单条新闻数据

```python
def get_one_news(news_tag):
    title_tag = news_tag.select_one('div > div:nth-child(2) > h3 > a')
    title = title_tag.text if title_tag else None 

    img_tag = news_tag.select_one('div > div.mr10 > a > img')
    img_link = urljoin('https://china.chinadaily.com.cn/',
                       img_tag.get('src')) if img_tag else None

    abstract_tag = news_tag.select_one('div > div:nth-child(2) > p')
    for b_tag in abstract_tag.select('b'): 
        b_tag.decompose()
    abstract = abstract_tag.text.strip() if abstract_tag else None

    content_link_tag = news_tag.select_one('div > div:nth-child(2) > h3 > a')
    content_link = urljoin('https://china.chinadaily.com.cn/',
                           content_link_tag.get('href')) if content_link_tag else None

    content_html = get_html(content_link)
    if content_html is None:
        content = None
    else:
        content = get_news_content(content_html)
    return title, img_link, abstract, content
```
**关键逻辑：**

- 使用CSS选择器定位标题、图片、摘要、链接标签
- ```urljoin
```
处理相对路径，转换为完整URL
- 通过```b_tag.decompose()
```
删除摘要中的日期标签
- 二次请求新闻详情页，提取正文内容
### 5. 提取页面所有新闻数据

```python
def get_data(bs):
    new_list_tag = bs.select_one('body > div.container > div.container-left2 > div')
    news_tags = new_list_tag.select('div.busBox3')
    data = [get_one_news(news_tag) for news_tag in news_tags]
    return data
```
**关键逻辑：**

- 先定位包含所有新闻的父容器
- 使用列表推导式批量处理每条新闻数据
### 6. 主程序逻辑

```python
if __name__ == '__main__':
    file_name = 'news.csv'
    url = 'https://china.chinadaily.com.cn/5bd5639ca3101a87ca8ff636/page_39.html'
    while True:
        html = get_html(url)
        if html is None:
            continue

        bs = BeautifulSoup(html, 'lxml')
        data = get_data(bs)

        next_page_tag = bs.select_one('#div_currpage > a:nth-child(13)')
        if next_page_tag is None:
            print('没有下一页')
            break

        url = urljoin('https://china.chinadaily.com.cn/',
                      next_page_tag.get('href'))
        save_data(data, file_name)
```
**关键逻辑：**

- 使用```while True
```
实现无限循环翻页
- 通过CSS选择器定位“下一页”按钮标签
- 自动拼接下一页URL，持续抓取直到无下一页链接
## 四、注意事项

1. 请遵守《网络安全法》及网站```robots.txt
```
协议，避免过度抓取
2. 建议在低峰时段运行爬虫，降低对目标网站的影响
3. 网站结构可能变更，需定期检查CSS选择器是否失效
4. 处理敏感数据时注意合规性，避免法律风险​
