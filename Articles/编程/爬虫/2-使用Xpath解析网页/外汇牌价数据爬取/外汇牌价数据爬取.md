源码：
![3. 抓取中行外汇牌价.py](./外汇牌价数据爬取.assert/1746936104698-a9d1230f-44e2-487c-81ee-2e9c8340ace3.py)

## 一、教程目标
本教程将详细讲解如何使用Python编写一个简单的爬虫程序，从中国银行官网抓取外汇牌价数据，并保存到CSV文件中。通过学习这个示例，你将掌握以下技能：

- 使用requests库发送HTTP请求
- 使用BeautifulSoup库解析HTML内容
- 使用pandas库处理和保存数据
- 处理网页编码问题## 二、环境准备
### 1. 安装必要的库
在开始之前，需要安装以下Python库：

```bash
pip install requests chardet pandas beautifulsoup4
```
### 2. 了解目标网站
我们的目标网站是中国银行外汇牌价页面：[https://www.boc.cn/sourcedb/whpj/](https://www.boc.cn/sourcedb/whpj/)该网站展示了各种货币的兑换信息，包括现汇买入价、现钞买入价等。我们将抓取多页数据，保存到CSV文件中。
## 三、核心代码解析
### 1. 发送HTTP请求获取HTML内容

```python
def get_html(url, params = None, timeout = 3):
    headers = {
        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0'
    }
    try:
        response = requests.get(url,
                                params=params,
                                headers=headers,
                                timeout=timeout)
        response.raise_for_status()
        response.encoding = chardet.detect(response.content)['encoding']
        return response.text
    except Exception as e:
        print(e)
        return None
    finally:
        time.sleep(2)
```
这段代码的关键点：

- 设置了User-Agent模拟浏览器访问
- 使用chardet自动检测网页编码
- 添加了timeout参数防止请求挂起
- 使用try-except处理异常情况
- 无论请求成功或失败，都暂停2秒（反爬策略）### 2. 从HTML中提取表格数据

```python
def get_data(html):
    bs = BeautifulSoup(html, 'lxml')
    table = bs.select('table')[1]
    trs = table.select('tr')
    data = []
    for tr in trs[1:]:
        tds= tr.select('td')
        td_data = [td.text for td in tds]
        data.append(td_data)
    return data
```
这段代码的关键点：

- 使用BeautifulSoup的select方法定位元素
- ```bs.select('table')[1]
```
表示获取页面上的第二个表格
- 遍历表格的每一行（跳过表头），提取单元格数据
- 使用列表推导式简化代码### 3. 保存数据到CSV文件

```python
def save_data(data, file_name):
    df = pd.DataFrame(data,
                      columns='货币名称	现汇买入价	现钞买入价	现汇卖出价	现钞卖出价	中行折算价	发布日期	发布时间'.split())
    if not os.path.exists(file_name):
        df.to_csv(file_name,
                  encoding='utf-8-sig',
                  header=True,
                  index=False)
    else:
        df.to_csv(file_name,
                  encoding='utf-8-sig',
                  header=False,
                  index=False,
                  mode='a')
```
这段代码的关键点：

- 使用pandas的DataFrame处理表格数据
- 自动创建表头（第一次写入时）
- 使用utf-8-sig编码确保中文正常显示
- 支持追加模式写入数据### 4. 主程序逻辑

```python
if __name__ == '__main__':
    urls = [f'https://www.boc.cn/sourcedb/whpj/index_{i}.html' for i in range(0, 6)]
    urls[0] = 'https://www.boc.cn/sourcedb/whpj/index.html'

    for url in urls:
        print(f'正在抓取页面：{url}')
        html = get_html(url)
        if html is None:
            print('抓取页面上失败')
            continue

        data = get_data(html)
        file_name = '外汇牌价.csv'
        save_data(data, file_name)
```
这段代码的关键点：

- 生成多个页面的URL（第1页和后续页URL格式不同）
- 遍历URL列表依次抓取数据
- 实现了基本的错误处理（跳过抓取失败的页面）
## 四、注意事项

1. 爬取网站数据时，请遵守网站的robots.txt规则和相关法律法规
2. 控制爬取频率，避免对目标网站造成压力
3. 建议在本地测试环境中运行，确认代码无误后再正式部署
4. 网站结构可能随时变化，需要定期维护爬虫代码​
