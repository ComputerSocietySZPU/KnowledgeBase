源码：![1.+Scrapy抓取中行外汇牌价.zip](./抓取外汇牌价数据.assert/1746941108218-793f0e71-c836-42dc-b282-7bc2288d8eda.zip)

## 一、Scrapy简介与安装
### （一）Scrapy是什么？
Scrapy是一个为了爬取网站数据、提取结构性数据而编写的应用框架。它可以应用在数据挖掘、信息处理或存储历史数据等一系列的程序中。其架构清晰，提供了强大的选择器机制（如XPath和CSS选择器）、中间件系统、数据管道等，能够高效地完成各种复杂的爬取任务。
### （二）安装Scrapy
在Anaconda命令行或PyCharm的Terminal中执行以下命令安装Scrapy：

```bash
pip install scrapy
```
安装完成后，可以通过以下命令验证安装是否成功：

```bash
scrapy version
```
## 二、核心概念与命令
### （一）Scrapy核心组件

1. **Spider**：负责定义如何爬取特定网站（或一组网站）的网页内容和如何从网页中提取结构化数据。
2. **Item Pipeline**：负责处理爬虫从网页中提取的数据项，主要包括清洗、验证、存储等操作。
3. **Downloader**：负责获取网页内容并返回给爬虫。
4. **Scheduler**：负责调度待爬取的请求。
5. **Downloader Middlewares**：位于Scrapy引擎和下载器之间的组件，主要处理引擎与下载器之间的请求及响应。
6. **Spider Middlewares**：位于Scrapy引擎和爬虫之间的组件，主要处理爬虫的输入（响应）和输出（请求）。### （二）常用命令

1. **创建项目**：
```bash
scrapy startproject [项目名]
```
例如：

```bash
scrapy startproject Currency
```

1. **生成蜘蛛**：
```bash
scrapy genspider [蜘蛛名][网址]
```
例如：

```bash
scrapy genspider BOCCurrency "https://www.boc.cn/sourcedb/whpj/index.html"
```

1. **测试选择器**：
```bash
scrapy shell [网址]
```
例如：

```bash
scrapy shell "https://www.boc.cn/sourcedb/whpj/index.html"
```
在shell中，可以使用以下命令测试选择器：

```python
response.css('选择器表达式')
response.xpath('选择器表达式')
```

1. **运行爬虫**：
```bash
scrapy crawl [蜘蛛名] -o [数据文件名]
```
例如：

```bash
scrapy crawl BOCCurrency -o currency.csv
```
## 三、项目实战：抓取中行外汇牌价
### （一）创建项目
在Anaconda命令行或PyCharm的Terminal中执行以下命令创建项目：

```bash
scrapy startproject Currency
```
这将创建一个名为"Currency"的项目，包含以下目录结构：

```plain
Currency/
    scrapy.cfg            # 部署配置文件
    Currency/             # 项目的Python模块，将从这里引用代码
        __init__.py
        items.py          # 项目的数据模型
        middlewares.py    # 项目的中间件
        pipelines.py      # 项目的数据管道
        settings.py       # 项目的设置
        spiders/          # 存放爬虫的目录
            __init__.py
```
### （二）生成蜘蛛
切换到项目目录，然后执行以下命令生成蜘蛛：

```bash
scrapy genspider BOCCurrency "https://www.boc.cn/sourcedb/whpj/index.html"
```
这将在"spiders"目录下创建一个名为"BOCCurrency.py"的文件，内容如下：

```python
import scrapy

class BoccurrencySpider(scrapy.Spider):
    name = "BOCCurrency"
    allowed_domains = ["www.boc.cn"]
    start_urls = ["https://www.boc.cn/sourcedb/whpj/index.html"]

    def parse(self, response):
        pass
```
### （三）定义数据项
打开"items.py"文件，定义要抓取的数据项：

```python
import scrapy

class CurrencyItem(scrapy.Item):
    name = scrapy.Field()  # 货币名称
    buying_rate = scrapy.Field()  # 现汇买入价
    cash_buying_rate = scrapy.Field()  # 现钞买入价
```
### （四）编写爬虫逻辑
打开"spiders/BOCCurrency.py"文件，修改```parse
```
方法以提取数据：

```python
from typing import Iterable
import scrapy
from scrapy import Request
from Currency.items import CurrencyItem

class BoccurrencySpider(scrapy.Spider):
    name = "BOCCurrency"
    allowed_domains = ["www.boc.cn"]
    start_urls = ["https://www.boc.cn/sourcedb/whpj/index.html"]

    def start_requests(self):
        """生成多个页面的请求，处理分页数据"""
        base_url = "https://www.boc.cn/sourcedb/whpj/index_{}.html"
        urls = [base_url.format(i) for i in range(1, 4)]
        urls[0] = "https://www.boc.cn/sourcedb/whpj/index.html"  # 第一页
        for url in urls:
            yield Request(url=url, callback=self.parse, dont_filter=True)  # 不去重，确保所有页面都被抓取

    def parse(self, response):
        """解析页面内容，提取数据"""
        table = response.css('body > div > div.BOC_main > div.publish > div:nth-child(3) > table')[0]
        tr_tags = table.css('tr')
        for tr_tag in tr_tags[1:]:  # 跳过标题行
            td_texts = tr_tag.css('td::text').getall()
            # 使用字典方式返回数据
            yield {
                'name': td_texts[0],
                'buying_rate': td_texts[1],
                'cash_buying_rate': td_texts[2]
            }
            # 或者使用Item类方式返回数据
            # item = CurrencyItem()
            # item['name'] = td_texts[0]
            # item['buying_rate'] = td_texts[1]
            # item['cash_buying_rate'] = td_texts[2]
            # yield item
```
### （五）配置项目设置
打开"settings.py"文件，进行以下配置：

1. **设置下载延时**：
```python
DOWNLOAD_DELAY = 2  # 设置下载延时为2秒，避免频繁请求被封IP
```

1. **配置User-Agent**：
```python
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
```

1. **启用数据管道**（如果需要自定义数据处理逻辑）：
```python
ITEM_PIPELINES = {
    'Currency.pipelines.CurrencyPipeline': 300,
}
```
### （六）（可选）自定义数据管道
打开"pipelines.py"文件，添加以下代码：

```python
import csv

class CurrencyPipeline:
    def __init__(self):
        self.file = open('currency_data.csv', 'w', newline='', encoding='utf-8')
        self.writer = csv.DictWriter(self.file, fieldnames=['name', 'buying_rate', 'cash_buying_rate'])
        self.writer.writeheader()

    def process_item(self, item, spider):
        self.writer.writerow(item)
        return item

    def close_spider(self, spider):
        self.file.close()
```
### （七）运行爬虫
在Terminal中执行以下命令运行爬虫：

```bash
scrapy crawl BOCCurrency -o currency.csv
```
这将启动爬虫并将数据保存到"currency.csv"文件中。如果已经配置了数据管道，也可以直接运行：

```bash
scrapy crawl BOCCurrency
```
## 四、高级技巧与常见问题解决
### （一）XPath与CSS选择器
Scrapy提供了强大的XPath和CSS选择器机制来定位和提取网页元素。在```scrapy shell
```
中可以方便地测试这些选择器：

1. **CSS选择器示例**：
```python
response.css('div.classname')  # 选择所有class为classname的div元素
response.css('a::attr(href)')  # 选择所有a标签的href属性
response.css('td::text').get()  # 获取第一个td标签的文本内容
response.css('td::text').getall()  # 获取所有td标签的文本内容
```

1. **XPath选择器示例**：
```python
response.xpath('//div[@class="classname"]')  # 选择所有class为classname的div元素
response.xpath('//a/@href')  # 选择所有a标签的href属性
response.xpath('//td/text()').get()  # 获取第一个td标签的文本内容
response.xpath('//td/text()').getall()  # 获取所有td标签的文本内容
```
### （二）处理分页数据
在爬虫中，经常需要处理分页数据。可以通过以下方式实现：

1. **在**```parse
```
**方法中生成下一页请求**：
```python
def parse(self, response):
    # 提取当前页数据
    # ...
    
    # 提取下一页链接
    next_page = response.css('a.next-page::attr(href)').get()
    if next_page:
        yield response.follow(next_page, self.parse)
```

1. **使用**```start_requests
```
**方法生成多页请求**（如本教程中的示例）：### （三）常见问题解决

1. **被网站封禁IP**：
- 设置适当的```DOWNLOAD_DELAY
```
。
- 使用代理IP池，可以通过自定义下载中间件实现。
1. **数据提取不准确**：
- 在```scrapy shell
```
中测试选择器表达式。
- 检查网页结构是否发生变化。
1. **爬虫运行缓慢**：
- 调整```CONCURRENT_REQUESTS
```
设置，增加并发请求数。
- 优化选择器表达式，减少不必要的处理。通过本教程，你已经学会了使用Scrapy框架进行数据抓取的基本流程和方法。在实际应用中，可以根据不同的网站结构和数据需求进行相应的调整和优化，以实现更高效、更稳定的数据抓取。
